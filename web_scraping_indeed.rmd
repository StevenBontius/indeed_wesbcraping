---
title: "Vacatures"
author: "Steven Bontius"
date: "3-3-2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(xfun)

packages <- c("tidyverse"    
             ,"rvest"   
             ,"xml2"
             ,"glue"
             ,"Hmisc"
             ,"skimr"
            )

xfun::pkg_attach2(packages, message = FALSE)
```



# Web scraping

It is impossible to use the &start to go to the next page. There is some anti 
web scraping active that prohibits this. A vjk=2ab3c3e72a3a2f2f variable is added 
to the url, which makes it unique. More investigation necessary, but for now there
is no time so doing it the semi manual way. Visiting the website and copying the 
urls. 

```{r Setting up for data extraction} 
job_ids <- list()
job_titles <- list()
job_snippets <- list()
company_names <- list()
company_locations <- list()
```


```{r scraping the urls}

page_url <- "url"


web_page <- read_html(page_url)
    
    
# Getting job ids    
job_id <- web_page %>% 
    html_nodes(".tapItem") %>% 
    html_attr("data-jk")
    
job_ids <- c(job_ids, job_id)

# Getting job titles
job_title <- web_page %>% 
    html_nodes(".jobTitle") %>% 
    html_text()
    
job_titles <- c(job_titles, job_title)

# Getting job snippet
job_snippet <- web_page %>% 
    html_nodes(".job-snippet") %>% 
    html_text()

job_snippets <- c(job_snippets, job_snippet)

# Getting company names
company_name <- web_page %>% 
    html_nodes(".companyName") %>%
    html_text()

company_names <- c(company_names, company_name)
    
# Getting company locations

company_location <- web_page %>% 
    html_nodes(".companyLocation") %>% 
    html_text()

company_locations <- c(company_locations, company_location)


```


```{r processing the scraped data and saving it to file}
# unlisting into data frame
vacancy_listings <- data.frame(unlist(job_ids) 
                               , unlist(job_titles)
                               , unlist(job_snippets)
                               , unlist(company_names)
                               , unlist(company_locations)
                               )
# renaming the columns
vacancy_listings <- vacancy_listings %>% 
    rename(job_id = unlist.job_ids.
           , job_title = unlist.job_titles.
           , job_snippet = unlist.job_snippets.
           , company_name = unlist.company_names.
           , company_location = unlist.company_locations.
           )
#reindexing rows
rownames(vacancies) <- 1:nrow(vacancies)

# writing to file commented out for safety reasons
# write.csv(vacancy_listings, "vacancy_listings.csv", row.names=FALSE)
```

# Missing data

The following job_id where missing, since there where only 15 missing manually 
searched the job descriptions. No clue why the rest of the data is there but
the job ids are missing.

70c50667036b8f27
aed9b019296ce1f9
058780ef8e25ee5d
5b6221be17f38439
c2bcd453f7dca0d6
b5a50f65248f7e14
9e03c4d1ed5a324e
46ef8f9b5a7d08e3
096be47b50d34672
08ff94886128bf44
b471e942ec4455d5
156643125d43971a
ee2feb1446a1d3a7
3ca0915b3e0f5ebc
f978375d051d1021

# Loading the modified file and removing the duplicates

```{r loading vacancies}
vacancies <- read.csv("vacancy_listings_final.csv")
full_job_listing <- readRDS("full_job_listing.RData")
```

# Loading complete job web page

```{r}
full_job_listing <- lapply(vacancies$job_id, function(job_id){
    job_page_url <- glue("https://nl.indeed.com/vacature-bekijken?jk={job_id}")
    job_page  <- read_html(job_page_url) %>%
        html_node(".jobsearch-jobDescriptionText") %>% 
        html_text2()
    Sys.sleep(2)
    return(job_page)
})

full_job_listing <- lapply(full_job_listing, function(page) {
    return (tolower(gsub("\\\n|\\,|\\:|\\;|\\-|\\)|\\(\\!", " ", page)))
})

#saveRDS(full_job_listing, file="full_job_listing.RData")
```

```{r}
job_salaries <- list()
company_ratings <- list()
company_no_reviews <- list()

for(i in 1:nrow(vacancies)) {
    Sys.sleep(2)
    job_page_url <- glue("https://nl.indeed.com/vacature-bekijken?jk={vacancies[i,1]}")
    web_page <- read_html(job_page_url)

    # extracting salary if available
    salary <- web_page %>% 
        html_node(xpath = '//*[@id="salaryInfoAndJobType"]') %>% 
        html_text()
    
    job_salaries <- c(job_salaries, salary)
    
    # extracting ratings if available
    rating <- web_page %>%
        html_node("[itemprop='ratingValue']") %>%
        html_attr("content") 
    
    company_ratings <- c(company_ratings, rating)
    
    # extracting number of reviews if available
    review <- web_page %>% 
        html_node("[itemprop='ratingCount']") %>%
        html_attr("content") 
    
    company_no_reviews <- c(company_no_reviews, review)
    
}

# Adding to vacancies

vacancies$job_salary <- unlist(job_salaries)
vacancies$company_rating <- unlist(company_ratings)
vacancies$company_no_reviews <- unlist(company_no_reviews)

#write.csv(vacancies, "vacancy_listings_final.csv", row.names=FALSE)
```

# Skills

## Hard skills
-sql
-tableau
-powerbi
-r
-scrum
-agile
-python
-azure
-excel
-qlik
-cognos
-spss
-ssis
-ssrs
-ssas
-git / github
-sap
-lean
-salesforce
-analytics
-cloud
-bigquery



## Soft skills
analytisch / analytical
communicatief / communicatieve / communicate / communicative 
inzicht / insight
present
klantgerichtheid / klantcontact / klanten/ customers / client
dashboards
stakeholders
impact
resultaatgerichtheid
ondernemend
analyseren
creativiteit / creatief
proactief / actief
adviseren / advies
zelfstandig / zelfstandigheid


## Educations
hbo / bachelor
wo / master
phd 

```{r}
# function to check for key words present in data

is_keyword_present <- function(keyword, data){
    return( if(grepl(paste(keyword, collapse = "|"), data) == TRUE) 
        1 else 0
    )
}

add_keyword <- function(keyword) {
    lapply(full_job_listing, function(full_page) {
      is_keyword_present(keyword, full_page)  
    })
}

# hard skills
vacancies$contains_sql          <- add_keyword("sql")
vacancies$contains_tableau      <- add_keyword("tableau")
vacancies$contains_powerbi      <- add_keyword(c("powerbi", "power_bi", "power-bi", "power bi"))
vacancies$contains_python       <- add_keyword("python")
vacancies$contains_scrum        <- add_keyword("scrum")
vacancies$contains_agile        <- add_keyword("agile")
vacancies$contains_azure        <- add_keyword("azure")
vacancies$contains_excel        <- add_keyword("\\bexcel\\b")
vacancies$contains_r            <- add_keyword("\\<r\\>")
vacancies$contains_qlik         <- add_keyword("qlik")
vacancies$contains_cognos       <- add_keyword("cognos")
vacancies$contains_lean         <- add_keyword("lean")
vacancies$contains_spss         <- add_keyword("spss")
vacancies$contains_ssis         <- add_keyword("ssis")
vacancies$conatins_ssrs         <- add_keyword("ssrs")
vacancies$contains_ssas         <- add_keyword("ssas")
vacancies$conatins_git          <- add_keyword(c("\\<git\\>", "github"))
vacancies$contains_sap          <- add_keyword("\\<sap\\>")
vacancies$contains_salesforce   <- add_keyword("salesforce")
vacancies$contains_analytics    <- add_keyword("analytics")
vacancies$contains_cloud        <- add_keyword("\\<cloud\\>")
vacancies$contains_bigquery     <- add_keyword(c("bigquery", "big query"))
vacancies$contains_dashboard    <- add_keyword("dashboard")

# soft skills
vacancies$contains_analytical   <- add_keyword(c("analitisch", "analytical"))    
vacancies$contains_communicative <- add_keyword(c("communicatief","communicatieve", "communicator", "communicative", "communicate"))
vacancies$contains_customer     <- add_keyword(c("klantgerichtheid", "klant", "customer"))
vacancies$contains_stakeholder  <- add_keyword("stakeholder")
vacancies$contains_impact       <- add_keyword("impact")
vacancies$contains_result       <- add_keyword("resultaatgerichtheid")
vacancies$contains_entrepeneur  <- add_keyword(c("ondernemend", "entrepreneur"))

                                               



creativiteit / creatief
proactief / actief
adviseren / advies
zelfstandig / zelfstandigheid




```



# Data cleaninig
- Removed Nederland as cit from CBS database since it interferes with the company_location nederland.
- Of the 540 scraped jobs only 400 have an unique job id.
- reindexed row numbers

# To be investigated
- Why does &start not work?
- Why does job_id sometimes become NA?

